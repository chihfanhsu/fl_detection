{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mmnet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "# google 的 NN coding 套件\n",
    "import tensorflow as tf\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''setting'''\n",
    "gpus = [1] # Here I set CUDA to only see one GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=','.join([str(i) for i in gpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './reg/'\n",
    "data_dir = './dataset/'\n",
    "logs_dir = model_dir + 'logs/'\n",
    "MAX_ITERATION = int(1e5 + 1)\n",
    "training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['img', 'pts'])\n"
     ]
    }
   ],
   "source": [
    "db_helen = pickle.load(open( data_dir+\"HELEN.pickle\", \"rb\" ) )\n",
    "print(db_helen.keys())\n",
    "#db_300W['img'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(x, train_phase, name='bn_layer'):\n",
    "    #with tf.variable_scope(name) as scope:\n",
    "    batch_norm = tf.layers.batch_normalization(\n",
    "            inputs=x,\n",
    "            momentum=0.9, epsilon=1e-5,\n",
    "            center=True, scale=True,\n",
    "            training = train_phase,\n",
    "            name=name\n",
    "    )\n",
    "    return batch_norm\n",
    "\n",
    "def conv_blk (inputs,n_filter, train_phase, name = 'conv_blk'):\n",
    "    with tf.variable_scope(name):\n",
    "        c1 = tf.layers.conv2d(inputs, filters=n_filter[0], kernel_size=[3,3], strides=(1,1), padding='same')       \n",
    "        c1_bn = batch_norm(c1, train_phase, name='c1_bn')\n",
    "        c1_relu = tf.nn.relu(c1_bn)\n",
    "        c2 = tf.layers.conv2d(c1_relu,filters=n_filter[1],kernel_size=[3,3],strides=(1,1),padding='same')        \n",
    "        c2_bn = batch_norm(c2, train_phase, name='c2_bn')\n",
    "        c2_relu = tf.nn.relu(c2_bn)\n",
    "        return c2_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define Model Input (x) and Output (y_),  y_ = f(x)\n",
    "x = tf.placeholder(tf.float32, [None, 224,224,3])\n",
    "y_ = tf.placeholder(tf.float32, [None,68,2]) # 136\n",
    "train_phase = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "y_one = tf.layers.flatten(y_)\n",
    "\n",
    "# convolutional part\n",
    "r1 = tf.layers.max_pooling2d(x,pool_size=[2,2],strides=(2,2))\n",
    "\n",
    "h1 = conv_blk(r1, [64,64], train_phase, name='conv_blk1')\n",
    "m1 = tf.layers.max_pooling2d(h1,pool_size=[2,2],strides=(2,2))\n",
    "\n",
    "h2 = conv_blk(m1, [128,128], train_phase, name='conv_blk2')\n",
    "m2 = tf.layers.max_pooling2d(h2,pool_size=[2,2],strides=(2,2))\n",
    "\n",
    "h3 = conv_blk(m2, [256,256], train_phase, name='conv_blk3')\n",
    "m3 = tf.layers.max_pooling2d(h3,pool_size=[2,2],strides=(2,2))\n",
    "\n",
    "h4 = conv_blk(m3, [512,512], train_phase, name='conv_blk4')\n",
    "m4 = tf.layers.max_pooling2d(h4,pool_size=[2,2],strides=(2,2))\n",
    "\n",
    "flt = tf.layers.flatten(m4)\n",
    "\n",
    "# fully connected part\n",
    "\n",
    "f1_do = tf.layers.dropout(flt,rate=0.5)\n",
    "\n",
    "f1 = tf.layers.dense(f1_do,256,activation=None)\n",
    "f1_bn = batch_norm(f1, train_phase, name='f1_bn')\n",
    "f1_relu = tf.nn.relu(f1_bn)\n",
    "\n",
    "f2 = tf.layers.dense(f1_relu,136,activation=None)\n",
    "\n",
    "y_out = tf.reshape(f2, shape = [-1,68,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_parameters 11147464\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "#         print(shape)\n",
    "#         print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "#             print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "#         print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print('total_parameters', total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Saver...\n"
     ]
    }
   ],
   "source": [
    "# Define the Model Loss (4)\n",
    "avg_losses = tf.reduce_mean(tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.squared_difference(y_, y_out),-1)),-1))\n",
    "\n",
    "# Define the Optimizer (5)\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(avg_losses)\n",
    "\n",
    "# y_pred = tf.argmax(tf.nn.softmax(y), 1, output_type=tf.int32)\n",
    "\n",
    "# # Accuracy of the Model\n",
    "# correct_prediction = tf.equal(y_pred, y_)\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# initialize the model\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "print(\"Setting up Saver...\")\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./reg/logs/model.ckpt-76000\n",
      "Loading sucessfully\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "if (training == False):\n",
    "    ckpt = tf.train.get_checkpoint_state(logs_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print('Loading sucessfully')\n",
    "    else:\n",
    "        print('No checkpoint file found')\n",
    "        raise\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data iterator\n",
    "def get_batch(X, Y, batch_size = 32):\n",
    "    # print ('shuffle training dataset')\n",
    "    idx = np.arange(len(X))    \n",
    "    while True:\n",
    "        np.random.shuffle(idx)\n",
    "        tb = int(len(X)/batch_size)\n",
    "        #print('total batches %d' % tb)\n",
    "        for b_idx in range(tb):\n",
    "            tar_idx = idx[(b_idx*batch_size):((b_idx+1)*batch_size)]\n",
    "            t_batch_x = X[tar_idx]\n",
    "            t_batch_y = Y[tar_idx]\n",
    "            # print(b_idx, t_batch_x.shape, t_batch_y.shape)\n",
    "            yield t_batch_x, t_batch_y\n",
    "\n",
    "def data_augmentation(images, pts, rot=(-30, 30), s=(0.6, 1.0)):\n",
    "    keypoints_on_images = []\n",
    "    for idx_img in range(images.shape[0]):\n",
    "        image = images[idx_img]\n",
    "        height, width = image.shape[0:2]\n",
    "        keypoints = []\n",
    "        for p in range(pts.shape[1]):\n",
    "            keypoints.append(ia.Keypoint(x=pts[idx_img,p,0], y=pts[idx_img,p,1]))\n",
    "        keypoints_on_images.append(ia.KeypointsOnImage(keypoints, shape=image.shape))\n",
    "\n",
    "    seq = iaa.Sequential([iaa.Affine(rotate=rot,scale=s)])\n",
    "    seq_det = seq.to_deterministic() # call this for each batch again, NOT only once at the start\n",
    "\n",
    "    # augment keypoints and images\n",
    "    images_aug = seq_det.augment_images(images)\n",
    "    keypoints_aug = seq_det.augment_keypoints(keypoints_on_images)\n",
    "    \n",
    "    pts_aug=[]\n",
    "    for img_idx, keypoints_after in enumerate(keypoints_aug):\n",
    "        img_pts_aug=[]\n",
    "        for kp_idx, keypoint in enumerate(keypoints_after.keypoints):\n",
    "            img_pts_aug.append([round(keypoint.x),round(keypoint.y)])\n",
    "        pts_aug.append(np.asarray(img_pts_aug))\n",
    "\n",
    "    pts_aug = np.asarray(pts_aug).astype(np.int32)\n",
    "    \n",
    "#     print('images_aug', images_aug.shape)\n",
    "#     print('pts_aug', pts_aug.shape)\n",
    "    return images_aug, pts_aug\n",
    "      \n",
    "# write image to file\n",
    "def write_result(batch_xs_valid, batch_pts, iter_num):\n",
    "    b = random.randint(0, batch_pts.shape[0]-1)\n",
    "    img = batch_xs_valid[b].copy()\n",
    "    pts = batch_pts[b] #print(pts)\n",
    "    for p in range(pts.shape[0]):\n",
    "        #print(\"p\",p, pts[p+1,0],pts[p+1,1])\n",
    "        cv2.circle(img,(pts[p,0],pts[p,1]), 2, (255,0,0), -1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    cv2.imwrite(model_dir + 'imgs/infer_'+str(iter_num)+'.png', img)\n",
    "    \n",
    "def eval_norm_error_image(infer, gt):\n",
    "    # loss of all landmarks\n",
    "    l2d = np.sum(np.sqrt(np.sum(np.square(infer-gt),axis=2)), axis=1)\n",
    "    # distance of eye corners\n",
    "    cd = np.sqrt(np.sum(np.square(gt[:,45,:]-gt[:,36,:]),axis=1))\n",
    "    norm_error_image = l2d/cd/68\n",
    "    return norm_error_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if training == True:\n",
    "    batches = get_batch(db_helen['img']['trainset'], db_helen['pts']['trainset'], batch_size = 32)\n",
    "    # Train Model for 1000 steps\n",
    "    hist_train_acc = []\n",
    "    hist_valid_acc = []\n",
    "    max_validloss = 99999\n",
    "    for step in range(MAX_ITERATION):\n",
    "        batch_xs, batch_ys = next(batches)\n",
    "        batch_xs_aug, batch_ys_aug =data_augmentation(batch_xs, batch_ys)\n",
    "\n",
    "        sess.run([extra_update_ops,train_step], feed_dict={x: batch_xs_aug, y_: batch_ys_aug, train_phase: True})\n",
    "\n",
    "        if (step % 500 == 0):\n",
    "            # get training accr\n",
    "            idx = np.arange(len(db_helen['img']['trainset']))    \n",
    "            tb = int(len(db_helen['img']['trainset'])/32)\n",
    "            acc_train= []\n",
    "            for b_idx in range(tb):\n",
    "                tar_idx = idx[(b_idx*32):((b_idx+1)*32)]\n",
    "                t_batch_x = db_helen['img']['trainset'][tar_idx]\n",
    "                t_batch_y = db_helen['pts']['trainset'][tar_idx]\n",
    "            acc_train.append(sess.run(avg_losses, feed_dict={x: t_batch_x, y_: t_batch_y, train_phase: False}))\n",
    "            print(\"[T] Step: %d, loss:%g\" % (step, np.mean(acc_train)))\n",
    "\n",
    "            #tar_idx = idx[(b_idx*32):((b_idx+1)*32)]\n",
    "            t_batch_x = db_helen['img']['testset']#[tar_idx]\n",
    "            t_batch_y = db_helen['pts']['testset']#[tar_idx]\n",
    "            infered_pts, acc_valid= sess.run([tf.reshape(y_out,shape=(-1,68,2)), avg_losses], feed_dict={x: t_batch_x,\n",
    "                                                                                                     y_: t_batch_y,\n",
    "                                                                                                     train_phase: False})\n",
    "    #             acc_valid.append(valid_loss)\n",
    "            write_result(db_helen['img']['testset'][np.arange(infered_pts.shape[0])], infered_pts, step)\n",
    "\n",
    "            if np.mean(acc_valid) < max_validloss:\n",
    "                saver.save(sess, logs_dir + \"model.ckpt\", step)\n",
    "                print(\"[V*] Step: %d, loss:%g\" % (step, np.mean(acc_valid)))\n",
    "                max_validloss = np.mean(acc_valid)\n",
    "            else:\n",
    "                print(\"[V] Step: %d, loss:%g\" % (step, np.mean(acc_valid)))\n",
    "\n",
    "            hist_train_acc.append(np.mean(acc_train))\n",
    "            hist_valid_acc.append(np.mean(acc_valid))\n",
    "else: # evaluate\n",
    "    t_batch_x = db_helen['img']['testset']#[tar_idx]\n",
    "    t_batch_y = db_helen['pts']['testset']#[tar_idx]\n",
    "    infered_pts, acc_valid= sess.run([tf.reshape(y_out,shape=(-1,68,2)), avg_losses], feed_dict={x: t_batch_x, y_: t_batch_y, train_phase: False})\n",
    "    norm_error_image = eval_norm_error_image(infered_pts, t_batch_y)\n",
    "    pandas.DataFrame({'loss':norm_error_image}).to_csv(model_dir + 'norm_error_image.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d = 3\n",
    "img = db_helen['img']['testset'][3]\n",
    "pts = t_batch_y = db_helen['pts']['testset'][3]\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "cv2.circle(img,(pts[36,0],pts[36,1]), 2, (255,0,0), -1)\n",
    "cv2.circle(img,(pts[45,0],pts[45,1]), 2, (255,0,0), -1)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
