{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import TensorflowUtils as utils\n",
    "import pickle\n",
    "import cv2\n",
    "import pandas\n",
    "import config\n",
    "import tqdm\n",
    "import time\n",
    "conf, _ = config.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''setting'''\n",
    "gpus = [conf.gpu] # Here I set CUDA to only see one GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=','.join([str(i) for i in gpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.tar_model == 'hybrid':\n",
    "    import model_hybrid as model\n",
    "else:\n",
    "    sys.exit(\"Sorry, Wrong Model!\")\n",
    "\n",
    "# load model\n",
    "model_dir = './' + conf.tar_model + '/'\n",
    "data_dir = './dataset/'\n",
    "logs_dir = model_dir+'logs/'\n",
    "imgs_dir = model_dir+'imgs/'\n",
    "pred_dir = model_dir+'pred/'\n",
    "NUM_OF_CLASSESS = int(68 + 1)\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "if not os.path.exists(imgs_dir):\n",
    "    os.makedirs(imgs_dir)\n",
    "if not os.path.exists(pred_dir):\n",
    "    os.makedirs(pred_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loss_val, var_list):\n",
    "    optimizer = tf.train.AdamOptimizer(conf.lr)\n",
    "    grads = optimizer.compute_gradients(loss_val, var_list=var_list)\n",
    "    if conf.debug:\n",
    "        # print(len(var_list))\n",
    "        for grad, var in grads:\n",
    "            utils.add_gradient_summary(grad, var)\n",
    "    return optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_helen = pickle.load(open(data_dir+\"combined.pickle\", \"rb\" ) )\n",
    "# print the data structure\n",
    "print(db_helen.keys())\n",
    "print(db_helen['trainset'].keys())\n",
    "# print the shape of tratining set\n",
    "print(db_helen['trainset']['pts'].shape)\n",
    "print(db_helen['trainset']['img'].shape)\n",
    "# print the shape of testing set\n",
    "print(db_helen['testset']['pts'].shape)\n",
    "print(db_helen['testset']['img'].shape)\n",
    "# declear data iterator\n",
    "train_batches = utils.get_batch(db_helen['trainset']['img'], db_helen['trainset']['pts'], batch_size = conf.batch_size)\n",
    "valid_batches = utils.get_batch(db_helen['testset']['img'], db_helen['testset']['pts'], batch_size = conf.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default() as g:\n",
    "    # model input\n",
    "    keep_probability = tf.placeholder(tf.float32, name=\"keep_probabilty\")\n",
    "    image = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, 3], name=\"input_image\")\n",
    "    y_ = tf.placeholder(tf.float32, [None,68,2]) # 136\n",
    "    annotation = tf.placeholder(tf.int32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, 1], name=\"annotation\")\n",
    "    train_phase = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "    # model structure\n",
    "    pred_annotation, logits, heat_map, pred_pts = model.inference(image, keep_probability, train_phase, conf.debug)\n",
    "    \n",
    "    \n",
    "\n",
    "    # heatmap loss\n",
    "    heatmap_loss = tf.reduce_mean((tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                          labels=tf.squeeze(annotation, squeeze_dims=[3]),\n",
    "                                                                          name=\"entropy\")))\n",
    "    # reg loss\n",
    "    reg_losses = tf.reduce_mean(tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.squared_difference(y_, pred_pts),-1)),-1))\n",
    "\n",
    "    tot_loss = heatmap_loss + 0.25*reg_losses\n",
    "    loss_summary = tf.summary.scalar(\"entropy\", tot_loss)\n",
    "    trainable_var = tf.trainable_variables()\n",
    "    if conf.debug:\n",
    "        for var in trainable_var:\n",
    "            utils.add_to_regularization_and_summary(var)\n",
    "\n",
    "    train_op = train(tot_loss, trainable_var)\n",
    "\n",
    "    print(\"Setting up summary op...\")\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    print(\"Setting up Saver...\")\n",
    "    saver = tf.train.Saver()\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    # check # of parameter\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        total_parameters += variable_parameters\n",
    "    print('total_parameters', total_parameters)\n",
    "    \n",
    "    # create session\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False), graph=g)\n",
    "    if (conf.training == False):\n",
    "        ckpt = tf.train.get_checkpoint_state(logs_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print('Loading sucessfully')\n",
    "        else:\n",
    "            print('No checkpoint file found')\n",
    "            raise\n",
    "    else:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "    # create two summary writers to show training loss and validation loss in the same graph\n",
    "    # need to create two folders 'train' and 'validation' inside FLAGS.logs_dir\n",
    "    train_writer = tf.summary.FileWriter(logs_dir+'/train', sess.graph)\n",
    "    validation_writer = tf.summary.FileWriter(logs_dir+'/validation', sess.graph)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "if conf.training == True:\n",
    "    stop_count = 0\n",
    "    itr = 0\n",
    "    fout= open(model_dir + 'LC_' + str(conf.testing)+ '.csv', 'w')\n",
    "    fout.write(str('step,train_t,train_l2,valid_t,valid_l2\\n'))\n",
    "    max_validloss = 99999\n",
    "    while True:\n",
    "        # prepare training input\n",
    "        batch_xs, batch_ys = next(train_batches)\n",
    "        batch_xs_aug, batch_ys_aug = utils.data_augmentation(batch_xs, batch_ys)\n",
    "        batch_ymap_aug = utils.pts2map(batch_ys_aug)\n",
    "\n",
    "        sess.run([train_op,extra_update_ops], feed_dict={image: batch_xs_aug/255,\n",
    "                                                         annotation: batch_ymap_aug,\n",
    "                                                         y_: batch_ys_aug,\n",
    "                                                         keep_probability: 0.5,\n",
    "                                                         train_phase:conf.training})\n",
    "\n",
    "        if itr % 500 == 0:\n",
    "            train_loss, train_l2 = sess.run([tot_loss,reg_losses], feed_dict={image: batch_xs_aug/255,\n",
    "                                                                                     annotation: batch_ymap_aug,\n",
    "                                                                                     y_: batch_ys_aug,\n",
    "                                                                                     keep_probability: 0.5,\n",
    "                                                                                     train_phase:conf.training})\n",
    "            print(\"[T] Step: %d, tot_loss:%g, L2_loss:%g\" % (itr, np.mean(train_loss),np.mean(train_l2)))\n",
    "            fout.write(str('%d,%.4f,%.4f,NA,NA\\n' % (itr, np.mean(train_loss), np.mean(train_l2))))\n",
    "            fout.flush()\n",
    "            # train_writer.add_summary(summary_str, itr)\n",
    "        # validation\n",
    "        if itr % 1000 == 0:\n",
    "            # prepare inputs\n",
    "            valid_losses = []\n",
    "            valid_l2_losses = []\n",
    "            for i in tqdm.trange(int((db_helen['testset']['pts'].shape[0])/conf.batch_size)):\n",
    "                batch_xs_valid, batch_ys_valid = next(valid_batches)\n",
    "                batch_ymap_valid = utils.pts2map(batch_ys_valid);# print(batch_ymap_valid.shape)\n",
    "\n",
    "                valid_loss,valid_l2, pts_maps=sess.run([tot_loss,\n",
    "                                                        reg_losses,\n",
    "                                                        heat_map], feed_dict={image: batch_xs_valid/255,\n",
    "                                                                                             annotation: batch_ymap_valid,\n",
    "                                                                                             y_: batch_ys_valid,\n",
    "                                                                                             keep_probability: 1.0,\n",
    "                                                                                             train_phase:conf.training})\n",
    "                valid_losses.append(valid_loss)\n",
    "                valid_l2_losses.append(valid_l2)\n",
    "            # write result figure to the imgs/\n",
    "            utils.write_result(batch_xs_valid, pts_maps, itr, imgs_dir)\n",
    "            # save validation log\n",
    "            # validation_writer.add_summary(summary_sva, itr)\n",
    "            # save the ckpt if reachings better loss\n",
    "            calc_v_loss = np.mean(valid_losses)\n",
    "            calc_l2_loss = np.mean(valid_l2_losses)\n",
    "            if calc_v_loss < max_validloss:\n",
    "                saver.save(sess, logs_dir + \"model.ckpt\", itr)\n",
    "                print(\"[V*] Step: %d, tot_loss:%g, l2_loss:%g\" % (itr, calc_v_loss, calc_l2_loss))\n",
    "                max_validloss = calc_v_loss\n",
    "                stop_count = 0\n",
    "            else:\n",
    "                print(\"[V] Step: %d, tot_loss:%g, l2_loss:%g\" % (itr, calc_v_loss, calc_l2_loss))\n",
    "                stop_count = stop_count + 1\n",
    "                if stop_count > (conf.stop_tor + 1):\n",
    "                    break;\n",
    "                \n",
    "            fout.write(str('%d,NA,NA,%.4f,%.4f\\n' % (itr, calc_v_loss, calc_l2_loss)))\n",
    "            fout.flush()\n",
    "            \n",
    "        itr = itr + 1\n",
    "    fout.close()\n",
    "else:\n",
    "    print(\"Testing\")\n",
    "    testing_batch = conf.batch_size\n",
    "    infered_pts = []\n",
    "    start_time = time.time()\n",
    "    for t in tqdm.trange(int(db_helen['testset']['img'].shape[0]/testing_batch)+1):\n",
    "        t_batch_x = db_helen['testset']['img'][(t*testing_batch):((t+1)*testing_batch)]\n",
    "        t_batch_y = db_helen['testset']['pts'][(t*testing_batch):((t+1)*testing_batch)]\n",
    "        if (t == (int(db_helen['testset']['img'].shape[0]/testing_batch)+1)):\n",
    "            t_batch_x = db_helen['testset']['img'][(t*testing_batch):]\n",
    "            t_batch_y = db_helen['testset']['pts'][(t*testing_batch):]\n",
    "\n",
    "        batch_map=sess.run(heat_map, feed_dict={image: t_batch_x/255, keep_probability: 1.0,train_phase:conf.training})\n",
    "        if (t == 0):\n",
    "            infered_pts = utils.map2pts(batch_map)\n",
    "        else:\n",
    "            infered_pts = np.concatenate((infered_pts, utils.map2pts(batch_map)), axis=0)\n",
    "    used_time = time.time()-start_time\n",
    "    print('Avg. inference time: %.4f' % (used_time/db_helen['testset']['img'].shape[0]))        \n",
    "    with open(model_dir + 'pts_'+ str(conf.testing)+ '.pickle', 'wb') as handle:\n",
    "        pickle.dump(infered_pts, handle)\n",
    "\n",
    "    for idx, content in enumerate(zip(db_helen['testset']['img'],infered_pts)):\n",
    "        img = content[0].copy()\n",
    "        for kp_idx, keypoint in enumerate(content[1]):\n",
    "            cv2.circle(img,(keypoint[0],keypoint[1]), 2, (0,255,0), -1)\n",
    "            \n",
    "        cv2.imwrite(pred_dir + str(idx)+ '.png', img) \n",
    "\n",
    "    norm_error_image, norm_error_image_eye = utils.eval_norm_error_image(infered_pts, db_helen['testset']['pts'])\n",
    "    pandas.DataFrame({'loss':norm_error_image,'loss_eye':norm_error_image_eye}).to_csv(model_dir + 'norm_error_image_' + str(conf.testing)+ '.csv')\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
