{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mmnet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "# google 的 NN coding 套件\n",
    "import tensorflow as tf\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''setting'''\n",
    "gpus = [0] # Here I set CUDA to only see one GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=','.join([str(i) for i in gpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './dataset/'\n",
    "logs_dir = './reg/logs/'\n",
    "MAX_ITERATION = int(1e5 + 1)\n",
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pts', 'img'])\n"
     ]
    }
   ],
   "source": [
    "db_helen = pickle.load(open( data_dir+\"HELEN.pickle\", \"rb\" ) )\n",
    "print(db_helen.keys())\n",
    "#db_300W['img'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(x, train_phase, name='bn_layer'):\n",
    "    #with tf.variable_scope(name) as scope:\n",
    "    batch_norm = tf.layers.batch_normalization(\n",
    "            inputs=x,\n",
    "            momentum=0.9, epsilon=1e-5,\n",
    "            center=True, scale=True,\n",
    "            training = train_phase,\n",
    "            name=name\n",
    "    )\n",
    "    return batch_norm\n",
    "\n",
    "def conv_blk (inputs,n_filter, train_phase, name = 'conv_blk'):\n",
    "    with tf.variable_scope(name):\n",
    "        c1 = tf.layers.conv2d(inputs, filters=n_filter[0], kernel_size=[3,3], strides=(1,1), padding='same')       \n",
    "        c1_bn = batch_norm(c1, train_phase, name='c1_bn')\n",
    "        c1_relu = tf.nn.relu(c1_bn)\n",
    "        c2 = tf.layers.conv2d(c1_relu,filters=n_filter[1],kernel_size=[3,3],strides=(1,1),padding='same')        \n",
    "        c2_bn = batch_norm(c2, train_phase, name='c2_bn')\n",
    "        c2_relu = tf.nn.relu(c2_bn)\n",
    "        return c2_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define Model Input (x) and Output (y_),  y_ = f(x)\n",
    "x = tf.placeholder(tf.float32, [None, 224,224,3])\n",
    "y_ = tf.placeholder(tf.int32, [None,68,2]) # 136\n",
    "train_phase = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "y_one = tf.layers.flatten(y_)\n",
    "\n",
    "# convolutional part\n",
    "r1 = tf.layers.max_pooling2d(x,pool_size=[2,2],strides=(2,2))\n",
    "\n",
    "h1 = conv_blk(r1, [64,64], train_phase, name='conv_blk1')\n",
    "m1 = tf.layers.max_pooling2d(h1,pool_size=[2,2],strides=(2,2))\n",
    "\n",
    "h2 = conv_blk(m1, [128,128], train_phase, name='conv_blk2')\n",
    "m2 = tf.layers.max_pooling2d(h2,pool_size=[2,2],strides=(2,2))\n",
    "\n",
    "h3 = conv_blk(m2, [256,256], train_phase, name='conv_blk3')\n",
    "m3 = tf.layers.max_pooling2d(h3,pool_size=[2,2],strides=(2,2))\n",
    "\n",
    "h4 = conv_blk(m3, [512,512], train_phase, name='conv_blk4')\n",
    "m4 = tf.layers.max_pooling2d(h4,pool_size=[2,2],strides=(2,2))\n",
    "\n",
    "flt = tf.layers.flatten(m4)\n",
    "\n",
    "# fully connected part\n",
    "\n",
    "f1_do = tf.layers.dropout(flt,rate=0.5)\n",
    "\n",
    "f1 = tf.layers.dense(f1_do,256,activation=None)\n",
    "f1_bn = batch_norm(f1, train_phase, name='f1_bn')\n",
    "f1_relu = tf.nn.relu(f1_bn)\n",
    "\n",
    "f2 = tf.layers.dense(f1_relu,136,activation=None)\n",
    "f2_bn = batch_norm(f2, train_phase, name='f2_bn')\n",
    "f2_relu = tf.nn.relu(f2_bn)\n",
    "\n",
    "y_out = tf.reshape(f2_relu, shape = [-1,68,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_parameters 11146952\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "#         print(shape)\n",
    "#         print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "#             print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "#         print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print('total_parameters', total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Saver...\n"
     ]
    }
   ],
   "source": [
    "# Define the Model Loss (4)\n",
    "avg_losses = tf.reduce_mean(tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.squared_difference(y_, y_out),-1)),-1))\n",
    "\n",
    "# Define the Optimizer (5)\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(avg_losses)\n",
    "\n",
    "# y_pred = tf.argmax(tf.nn.softmax(y), 1, output_type=tf.int32)\n",
    "\n",
    "# # Accuracy of the Model\n",
    "# correct_prediction = tf.equal(y_pred, y_)\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# initialize the model\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "print(\"Setting up Saver...\")\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "if (training == False):\n",
    "    ckpt = tf.train.get_checkpoint_state(logs_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print('Loading sucessfully')\n",
    "    else:\n",
    "        print('No checkpoint file found')\n",
    "        raise\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data iterator\n",
    "def get_batch(X, Y, batch_size = 32):\n",
    "    # print ('shuffle training dataset')\n",
    "    idx = np.arange(len(X))    \n",
    "    while True:\n",
    "        np.random.shuffle(idx)\n",
    "        tb = int(len(X)/batch_size)\n",
    "        #print('total batches %d' % tb)\n",
    "        for b_idx in range(tb):\n",
    "            tar_idx = idx[(b_idx*batch_size):((b_idx+1)*batch_size)]\n",
    "            t_batch_x = X[tar_idx]\n",
    "            t_batch_y = Y[tar_idx]\n",
    "            # print(b_idx, t_batch_x.shape, t_batch_y.shape)\n",
    "            yield t_batch_x, t_batch_y\n",
    "\n",
    "def data_augmentation(images, pts, rot=(-30, 30), s=(0.6, 1.0)):\n",
    "    keypoints_on_images = []\n",
    "    for idx_img in range(images.shape[0]):\n",
    "        image = images[idx_img]\n",
    "        height, width = image.shape[0:2]\n",
    "        keypoints = []\n",
    "        for p in range(pts.shape[1]):\n",
    "            keypoints.append(ia.Keypoint(x=pts[idx_img,p,0], y=pts[idx_img,p,1]))\n",
    "        keypoints_on_images.append(ia.KeypointsOnImage(keypoints, shape=image.shape))\n",
    "\n",
    "    seq = iaa.Sequential([iaa.Affine(rotate=rot,scale=s)])\n",
    "    seq_det = seq.to_deterministic() # call this for each batch again, NOT only once at the start\n",
    "\n",
    "    # augment keypoints and images\n",
    "    images_aug = seq_det.augment_images(images)\n",
    "    keypoints_aug = seq_det.augment_keypoints(keypoints_on_images)\n",
    "    \n",
    "    pts_aug=[]\n",
    "    for img_idx, keypoints_after in enumerate(keypoints_aug):\n",
    "        img_pts_aug=[]\n",
    "        for kp_idx, keypoint in enumerate(keypoints_after.keypoints):\n",
    "            img_pts_aug.append([round(keypoint.x),round(keypoint.y)])\n",
    "        pts_aug.append(np.asarray(img_pts_aug))\n",
    "\n",
    "    pts_aug = np.asarray(pts_aug).astype(np.int32)\n",
    "    \n",
    "#     print('images_aug', images_aug.shape)\n",
    "#     print('pts_aug', pts_aug.shape)\n",
    "    return images_aug, pts_aug\n",
    "      \n",
    "# write image to file\n",
    "def write_result(batch_xs_valid, batch_pts, iter_num):\n",
    "    b = random.randint(0, batch_pts.shape[0]-1)\n",
    "    img = batch_xs_valid[b].copy()\n",
    "    pts = batch_pts[b] #print(pts)\n",
    "    for p in range(pts.shape[0]):\n",
    "        #print(\"p\",p, pts[p+1,0],pts[p+1,1])\n",
    "        cv2.circle(img,(pts[p,0],pts[p,1]), 2, (255,0,0), -1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    cv2.imwrite('./dan/imgs/infer_'+str(iter_num)+'.png', img)\n",
    "    \n",
    "def eval_norm_error_image(infer, gt):\n",
    "    # loss of all landmarks\n",
    "    l2d = np.sum(np.sqrt(np.sum(np.square(infer-gt),axis=2)), axis=1)\n",
    "    # distance of eye corners\n",
    "    cd = np.sqrt(np.sum(np.square(gt[:,45,:]-gt[:,36,:]),axis=1))\n",
    "    norm_error_image = l2d/cd/68\n",
    "    return norm_error_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T] Step: 0, loss:12344.6\n",
      "[V*] Step: 0, loss:12410.7\n",
      "[T] Step: 500, loss:43.8303\n",
      "[V*] Step: 500, loss:55.9306\n",
      "[T] Step: 1000, loss:24.5308\n",
      "[V*] Step: 1000, loss:35.467\n",
      "[T] Step: 1500, loss:29.3264\n",
      "[V] Step: 1500, loss:35.9701\n",
      "[T] Step: 2000, loss:20.3908\n",
      "[V*] Step: 2000, loss:29.8473\n",
      "[T] Step: 2500, loss:15.3907\n",
      "[V*] Step: 2500, loss:24.5196\n",
      "[T] Step: 3000, loss:17.8888\n",
      "[V] Step: 3000, loss:27.3354\n",
      "[T] Step: 3500, loss:12.7265\n",
      "[V*] Step: 3500, loss:20.932\n",
      "[T] Step: 4000, loss:10.5315\n",
      "[V*] Step: 4000, loss:19.7039\n",
      "[T] Step: 4500, loss:10.2499\n",
      "[V*] Step: 4500, loss:17.4968\n",
      "[T] Step: 5000, loss:9.44238\n",
      "[V] Step: 5000, loss:18.1448\n",
      "[T] Step: 5500, loss:9.53745\n",
      "[V] Step: 5500, loss:17.6747\n",
      "[T] Step: 6000, loss:9.27972\n",
      "[V*] Step: 6000, loss:17.3581\n",
      "[T] Step: 6500, loss:8.49145\n",
      "[V] Step: 6500, loss:17.7582\n",
      "[T] Step: 7000, loss:8.33274\n",
      "[V*] Step: 7000, loss:16.558\n",
      "[T] Step: 7500, loss:9.88514\n",
      "[V] Step: 7500, loss:18.562\n",
      "[T] Step: 8000, loss:10.5001\n",
      "[V] Step: 8000, loss:19.7434\n",
      "[T] Step: 8500, loss:10.5185\n",
      "[V] Step: 8500, loss:19.1131\n",
      "[T] Step: 9000, loss:7.41476\n",
      "[V] Step: 9000, loss:17.0209\n",
      "[T] Step: 9500, loss:7.56124\n",
      "[V] Step: 9500, loss:17.0189\n",
      "[T] Step: 10000, loss:6.48008\n",
      "[V*] Step: 10000, loss:16.4269\n",
      "[T] Step: 10500, loss:10.1219\n",
      "[V] Step: 10500, loss:19.9939\n",
      "[T] Step: 11000, loss:6.87258\n",
      "[V*] Step: 11000, loss:15.9481\n",
      "[T] Step: 11500, loss:7.41748\n",
      "[V] Step: 11500, loss:17.6535\n",
      "[T] Step: 12000, loss:5.9801\n",
      "[V] Step: 12000, loss:16.9206\n",
      "[T] Step: 12500, loss:6.44696\n",
      "[V] Step: 12500, loss:16.6079\n",
      "[T] Step: 13000, loss:5.39547\n",
      "[V*] Step: 13000, loss:15.1435\n",
      "[T] Step: 13500, loss:4.54007\n",
      "[V] Step: 13500, loss:16.9503\n",
      "[T] Step: 14000, loss:5.00585\n",
      "[V] Step: 14000, loss:17.8575\n",
      "[T] Step: 14500, loss:5.77716\n",
      "[V] Step: 14500, loss:17.0985\n",
      "[T] Step: 15000, loss:4.49024\n",
      "[V] Step: 15000, loss:16.9553\n",
      "[T] Step: 15500, loss:6.26789\n",
      "[V] Step: 15500, loss:16.9802\n",
      "[T] Step: 16000, loss:3.82144\n",
      "[V] Step: 16000, loss:16.3743\n",
      "[T] Step: 16500, loss:4.16248\n",
      "[V] Step: 16500, loss:16.5291\n",
      "[T] Step: 17000, loss:3.67395\n",
      "[V] Step: 17000, loss:16.9869\n",
      "[T] Step: 17500, loss:4.05959\n",
      "[V] Step: 17500, loss:16.5622\n",
      "[T] Step: 18000, loss:4.51258\n",
      "[V] Step: 18000, loss:16.1681\n",
      "[T] Step: 18500, loss:4.37991\n",
      "[V] Step: 18500, loss:18.0913\n",
      "[T] Step: 19000, loss:3.33607\n",
      "[V] Step: 19000, loss:15.9621\n",
      "[T] Step: 19500, loss:3.41777\n",
      "[V] Step: 19500, loss:17.19\n",
      "[T] Step: 20000, loss:3.2153\n",
      "[V] Step: 20000, loss:16.7826\n",
      "[T] Step: 20500, loss:3.64299\n",
      "[V] Step: 20500, loss:16.018\n",
      "[T] Step: 21000, loss:4.48353\n",
      "[V] Step: 21000, loss:17.1415\n",
      "[T] Step: 21500, loss:2.98204\n",
      "[V] Step: 21500, loss:17.0561\n",
      "[T] Step: 22000, loss:3.22996\n",
      "[V] Step: 22000, loss:16.8183\n",
      "[T] Step: 22500, loss:4.03981\n",
      "[V] Step: 22500, loss:17.1123\n",
      "[T] Step: 23000, loss:3.62796\n",
      "[V] Step: 23000, loss:16.9919\n",
      "[T] Step: 23500, loss:2.96183\n",
      "[V] Step: 23500, loss:17.1019\n",
      "[T] Step: 24000, loss:3.45351\n",
      "[V] Step: 24000, loss:18.0745\n",
      "[T] Step: 24500, loss:2.49158\n",
      "[V] Step: 24500, loss:17.3993\n",
      "[T] Step: 25000, loss:2.46999\n",
      "[V] Step: 25000, loss:17.7503\n",
      "[T] Step: 25500, loss:4.69647\n",
      "[V] Step: 25500, loss:18.7828\n",
      "[T] Step: 26000, loss:2.45703\n",
      "[V] Step: 26000, loss:17.586\n",
      "[T] Step: 26500, loss:2.39463\n",
      "[V] Step: 26500, loss:16.4969\n",
      "[T] Step: 27000, loss:2.3743\n",
      "[V] Step: 27000, loss:17.5573\n",
      "[T] Step: 27500, loss:3.03012\n",
      "[V] Step: 27500, loss:18.8798\n",
      "[T] Step: 28000, loss:2.44667\n",
      "[V] Step: 28000, loss:18.7006\n",
      "[T] Step: 28500, loss:1.88541\n",
      "[V] Step: 28500, loss:17.132\n",
      "[T] Step: 29000, loss:3.26406\n",
      "[V] Step: 29000, loss:17.4595\n",
      "[T] Step: 29500, loss:2.11843\n",
      "[V] Step: 29500, loss:18.4871\n",
      "[T] Step: 30000, loss:1.84639\n",
      "[V] Step: 30000, loss:18.3424\n",
      "[T] Step: 30500, loss:2.32201\n",
      "[V] Step: 30500, loss:18.3554\n",
      "[T] Step: 31000, loss:2.19473\n",
      "[V] Step: 31000, loss:19.6254\n",
      "[T] Step: 31500, loss:2.59551\n",
      "[V] Step: 31500, loss:18.6343\n",
      "[T] Step: 32000, loss:3.05703\n",
      "[V] Step: 32000, loss:19.3701\n",
      "[T] Step: 32500, loss:1.92943\n",
      "[V] Step: 32500, loss:17.1436\n",
      "[T] Step: 33000, loss:2.37596\n",
      "[V] Step: 33000, loss:18.1974\n",
      "[T] Step: 33500, loss:2.74018\n",
      "[V] Step: 33500, loss:19.7645\n",
      "[T] Step: 34000, loss:2.54688\n",
      "[V] Step: 34000, loss:19.9274\n",
      "[T] Step: 34500, loss:1.75434\n",
      "[V] Step: 34500, loss:18.3272\n",
      "[T] Step: 35000, loss:2.20294\n",
      "[V] Step: 35000, loss:18.7632\n",
      "[T] Step: 35500, loss:1.81909\n",
      "[V] Step: 35500, loss:18.9394\n",
      "[T] Step: 36000, loss:2.02398\n",
      "[V] Step: 36000, loss:18.7786\n",
      "[T] Step: 36500, loss:1.77704\n",
      "[V] Step: 36500, loss:18.2717\n",
      "[T] Step: 37000, loss:1.58065\n",
      "[V] Step: 37000, loss:18.5405\n",
      "[T] Step: 37500, loss:1.85765\n",
      "[V] Step: 37500, loss:18.8605\n",
      "[T] Step: 38000, loss:1.6699\n",
      "[V] Step: 38000, loss:19.1135\n",
      "[T] Step: 38500, loss:1.74025\n",
      "[V] Step: 38500, loss:20.4411\n",
      "[T] Step: 39000, loss:2.02938\n",
      "[V] Step: 39000, loss:19.9455\n",
      "[T] Step: 39500, loss:1.71945\n",
      "[V] Step: 39500, loss:20.9784\n",
      "[T] Step: 40000, loss:1.7015\n",
      "[V] Step: 40000, loss:20.0112\n",
      "[T] Step: 40500, loss:2.1525\n",
      "[V] Step: 40500, loss:20.2468\n",
      "[T] Step: 41000, loss:2.56224\n",
      "[V] Step: 41000, loss:21.2374\n",
      "[T] Step: 41500, loss:1.89108\n",
      "[V] Step: 41500, loss:22.4969\n",
      "[T] Step: 42000, loss:1.45501\n",
      "[V] Step: 42000, loss:20.0494\n",
      "[T] Step: 42500, loss:2.26217\n",
      "[V] Step: 42500, loss:19.7816\n",
      "[T] Step: 43000, loss:1.56023\n",
      "[V] Step: 43000, loss:19.9622\n",
      "[T] Step: 43500, loss:1.43578\n",
      "[V] Step: 43500, loss:20.6767\n",
      "[T] Step: 44000, loss:1.64609\n",
      "[V] Step: 44000, loss:22.1745\n",
      "[T] Step: 44500, loss:1.684\n",
      "[V] Step: 44500, loss:23.0864\n",
      "[T] Step: 45000, loss:1.63156\n",
      "[V] Step: 45000, loss:20.6281\n",
      "[T] Step: 45500, loss:2.49601\n",
      "[V] Step: 45500, loss:21.3526\n",
      "[T] Step: 46000, loss:1.78269\n",
      "[V] Step: 46000, loss:22.1396\n",
      "[T] Step: 46500, loss:1.60897\n",
      "[V] Step: 46500, loss:21.2947\n",
      "[T] Step: 47000, loss:1.37964\n",
      "[V] Step: 47000, loss:20.951\n",
      "[T] Step: 47500, loss:1.49903\n",
      "[V] Step: 47500, loss:23.0841\n",
      "[T] Step: 48000, loss:1.44767\n",
      "[V] Step: 48000, loss:20.9702\n",
      "[T] Step: 48500, loss:1.30985\n",
      "[V] Step: 48500, loss:20.4787\n",
      "[T] Step: 49000, loss:1.67782\n",
      "[V] Step: 49000, loss:21.4614\n",
      "[T] Step: 49500, loss:1.59348\n",
      "[V] Step: 49500, loss:21.8669\n",
      "[T] Step: 50000, loss:2.16609\n",
      "[V] Step: 50000, loss:24.0017\n",
      "[T] Step: 50500, loss:1.20705\n",
      "[V] Step: 50500, loss:22.9086\n",
      "[T] Step: 51000, loss:1.41004\n",
      "[V] Step: 51000, loss:21.8026\n",
      "[T] Step: 51500, loss:2.05989\n",
      "[V] Step: 51500, loss:22.2267\n",
      "[T] Step: 52000, loss:1.56923\n",
      "[V] Step: 52000, loss:23.6356\n",
      "[T] Step: 52500, loss:1.39871\n",
      "[V] Step: 52500, loss:21.6388\n",
      "[T] Step: 53000, loss:1.30253\n",
      "[V] Step: 53000, loss:23.4292\n",
      "[T] Step: 53500, loss:1.53743\n",
      "[V] Step: 53500, loss:22.3592\n",
      "[T] Step: 54000, loss:1.54338\n",
      "[V] Step: 54000, loss:22.2697\n",
      "[T] Step: 54500, loss:1.38636\n",
      "[V] Step: 54500, loss:23.6419\n",
      "[T] Step: 55000, loss:1.43484\n",
      "[V] Step: 55000, loss:24.4259\n",
      "[T] Step: 55500, loss:1.46277\n",
      "[V] Step: 55500, loss:20.991\n",
      "[T] Step: 56000, loss:1.27799\n",
      "[V] Step: 56000, loss:22.9024\n",
      "[T] Step: 56500, loss:1.52435\n",
      "[V] Step: 56500, loss:21.4319\n",
      "[T] Step: 57000, loss:1.95814\n",
      "[V] Step: 57000, loss:23.0912\n",
      "[T] Step: 57500, loss:1.34362\n",
      "[V] Step: 57500, loss:22.1342\n",
      "[T] Step: 58000, loss:1.32869\n",
      "[V] Step: 58000, loss:22.4742\n",
      "[T] Step: 58500, loss:1.21221\n",
      "[V] Step: 58500, loss:23.4578\n",
      "[T] Step: 59000, loss:1.30261\n",
      "[V] Step: 59000, loss:22.9456\n",
      "[T] Step: 59500, loss:1.15408\n",
      "[V] Step: 59500, loss:22.8485\n",
      "[T] Step: 60000, loss:1.30634\n",
      "[V] Step: 60000, loss:24.1629\n",
      "[T] Step: 60500, loss:1.09908\n",
      "[V] Step: 60500, loss:24.6232\n",
      "[T] Step: 61000, loss:1.332\n",
      "[V] Step: 61000, loss:24.2034\n",
      "[T] Step: 61500, loss:1.26158\n",
      "[V] Step: 61500, loss:26.6223\n",
      "[T] Step: 62000, loss:1.277\n",
      "[V] Step: 62000, loss:24.0229\n",
      "[T] Step: 62500, loss:1.46814\n",
      "[V] Step: 62500, loss:25.1781\n",
      "[T] Step: 63000, loss:1.26307\n",
      "[V] Step: 63000, loss:24.6316\n",
      "[T] Step: 63500, loss:1.19337\n",
      "[V] Step: 63500, loss:24.9511\n",
      "[T] Step: 64000, loss:1.22973\n",
      "[V] Step: 64000, loss:24.9662\n",
      "[T] Step: 64500, loss:1.2079\n",
      "[V] Step: 64500, loss:25.5411\n",
      "[T] Step: 65000, loss:1.44114\n",
      "[V] Step: 65000, loss:27.979\n",
      "[T] Step: 65500, loss:1.57553\n",
      "[V] Step: 65500, loss:27.0988\n",
      "[T] Step: 66000, loss:1.27086\n",
      "[V] Step: 66000, loss:26.3656\n",
      "[T] Step: 66500, loss:1.03235\n",
      "[V] Step: 66500, loss:24.1594\n",
      "[T] Step: 67000, loss:1.05848\n",
      "[V] Step: 67000, loss:24.6051\n",
      "[T] Step: 67500, loss:1.14138\n",
      "[V] Step: 67500, loss:25.8121\n",
      "[T] Step: 68000, loss:1.19478\n",
      "[V] Step: 68000, loss:26.0415\n",
      "[T] Step: 68500, loss:1.07981\n",
      "[V] Step: 68500, loss:26.8056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T] Step: 69000, loss:1.20927\n",
      "[V] Step: 69000, loss:26.981\n",
      "[T] Step: 69500, loss:1.14653\n",
      "[V] Step: 69500, loss:26.7405\n",
      "[T] Step: 70000, loss:1.11439\n",
      "[V] Step: 70000, loss:26.1679\n",
      "[T] Step: 70500, loss:1.07165\n",
      "[V] Step: 70500, loss:27.0182\n",
      "[T] Step: 71000, loss:0.989983\n",
      "[V] Step: 71000, loss:28.5727\n",
      "[T] Step: 71500, loss:1.05708\n",
      "[V] Step: 71500, loss:25.8396\n",
      "[T] Step: 72000, loss:1.25301\n",
      "[V] Step: 72000, loss:27.7643\n",
      "[T] Step: 72500, loss:1.22142\n",
      "[V] Step: 72500, loss:24.9511\n",
      "[T] Step: 73000, loss:0.972442\n",
      "[V] Step: 73000, loss:26.06\n",
      "[T] Step: 73500, loss:1.02358\n",
      "[V] Step: 73500, loss:25.8425\n",
      "[T] Step: 74000, loss:1.33448\n",
      "[V] Step: 74000, loss:29.3094\n",
      "[T] Step: 74500, loss:1.01085\n",
      "[V] Step: 74500, loss:26.8563\n",
      "[T] Step: 75000, loss:1.05141\n",
      "[V] Step: 75000, loss:27.7609\n",
      "[T] Step: 75500, loss:1.06229\n",
      "[V] Step: 75500, loss:27.0757\n",
      "[T] Step: 76000, loss:1.00408\n",
      "[V] Step: 76000, loss:27.3724\n",
      "[T] Step: 76500, loss:2.35254\n",
      "[V] Step: 76500, loss:32.3688\n",
      "[T] Step: 77000, loss:1.19089\n",
      "[V] Step: 77000, loss:29.128\n",
      "[T] Step: 77500, loss:1.06063\n",
      "[V] Step: 77500, loss:25.4521\n",
      "[T] Step: 78000, loss:1.05428\n",
      "[V] Step: 78000, loss:28.0965\n",
      "[T] Step: 78500, loss:1.29195\n",
      "[V] Step: 78500, loss:29.9887\n",
      "[T] Step: 79000, loss:1.10539\n",
      "[V] Step: 79000, loss:27.2757\n",
      "[T] Step: 79500, loss:1.57036\n",
      "[V] Step: 79500, loss:30.2782\n",
      "[T] Step: 80000, loss:1.34793\n",
      "[V] Step: 80000, loss:30.068\n",
      "[T] Step: 80500, loss:1.19302\n",
      "[V] Step: 80500, loss:27.9206\n",
      "[T] Step: 81000, loss:1.17164\n",
      "[V] Step: 81000, loss:29.4602\n",
      "[T] Step: 81500, loss:1.43745\n",
      "[V] Step: 81500, loss:29.825\n",
      "[T] Step: 82000, loss:1.13885\n",
      "[V] Step: 82000, loss:29.9573\n",
      "[T] Step: 82500, loss:0.938002\n",
      "[V] Step: 82500, loss:30.8368\n",
      "[T] Step: 83000, loss:1.19994\n",
      "[V] Step: 83000, loss:33.3923\n",
      "[T] Step: 83500, loss:0.841184\n",
      "[V] Step: 83500, loss:31.3964\n",
      "[T] Step: 84000, loss:1.17682\n",
      "[V] Step: 84000, loss:31.2642\n",
      "[T] Step: 84500, loss:0.895645\n",
      "[V] Step: 84500, loss:30.6391\n",
      "[T] Step: 85000, loss:0.976657\n",
      "[V] Step: 85000, loss:29.6634\n",
      "[T] Step: 85500, loss:0.798762\n",
      "[V] Step: 85500, loss:29.602\n",
      "[T] Step: 86000, loss:0.976049\n",
      "[V] Step: 86000, loss:31.1185\n",
      "[T] Step: 86500, loss:1.14913\n",
      "[V] Step: 86500, loss:33.1355\n",
      "[T] Step: 87000, loss:1.3183\n",
      "[V] Step: 87000, loss:32.3959\n",
      "[T] Step: 87500, loss:0.836428\n",
      "[V] Step: 87500, loss:32.1786\n",
      "[T] Step: 88000, loss:0.833318\n",
      "[V] Step: 88000, loss:32.5066\n",
      "[T] Step: 88500, loss:0.936642\n",
      "[V] Step: 88500, loss:32.3976\n",
      "[T] Step: 89000, loss:0.99899\n",
      "[V] Step: 89000, loss:29.7742\n",
      "[T] Step: 89500, loss:0.891031\n",
      "[V] Step: 89500, loss:31.083\n",
      "[T] Step: 90000, loss:0.882137\n",
      "[V] Step: 90000, loss:31.0168\n",
      "[T] Step: 90500, loss:0.793507\n",
      "[V] Step: 90500, loss:31.6607\n",
      "[T] Step: 91000, loss:1.09542\n",
      "[V] Step: 91000, loss:34.8135\n",
      "[T] Step: 91500, loss:0.912245\n",
      "[V] Step: 91500, loss:35.0451\n",
      "[T] Step: 92000, loss:0.925582\n",
      "[V] Step: 92000, loss:33.8079\n",
      "[T] Step: 92500, loss:1.24385\n",
      "[V] Step: 92500, loss:31.7563\n",
      "[T] Step: 93000, loss:0.911054\n",
      "[V] Step: 93000, loss:34.0849\n",
      "[T] Step: 93500, loss:1.04517\n",
      "[V] Step: 93500, loss:34.0317\n",
      "[T] Step: 94000, loss:0.824929\n",
      "[V] Step: 94000, loss:34.691\n",
      "[T] Step: 94500, loss:0.869949\n",
      "[V] Step: 94500, loss:33.9783\n",
      "[T] Step: 95000, loss:1.15903\n",
      "[V] Step: 95000, loss:36.7446\n",
      "[T] Step: 95500, loss:1.24032\n",
      "[V] Step: 95500, loss:35.5484\n",
      "[T] Step: 96000, loss:0.999049\n",
      "[V] Step: 96000, loss:35.9228\n",
      "[T] Step: 96500, loss:0.846845\n",
      "[V] Step: 96500, loss:35.7335\n",
      "[T] Step: 97000, loss:1.58955\n",
      "[V] Step: 97000, loss:36.4378\n",
      "[T] Step: 97500, loss:0.974967\n",
      "[V] Step: 97500, loss:36.4097\n",
      "[T] Step: 98000, loss:0.960896\n",
      "[V] Step: 98000, loss:36.9582\n",
      "[T] Step: 98500, loss:0.86687\n",
      "[V] Step: 98500, loss:36.4069\n",
      "[T] Step: 99000, loss:0.911202\n",
      "[V] Step: 99000, loss:35.4714\n",
      "[T] Step: 99500, loss:0.901341\n",
      "[V] Step: 99500, loss:35.0765\n",
      "[T] Step: 100000, loss:0.986102\n",
      "[V] Step: 100000, loss:38.3031\n"
     ]
    }
   ],
   "source": [
    "if training == True:\n",
    "    batches = get_batch(db_helen['img']['trainset'], db_helen['pts']['trainset'], batch_size = 32)\n",
    "    # Train Model for 1000 steps\n",
    "    hist_train_acc = []\n",
    "    hist_valid_acc = []\n",
    "    max_validloss = 99999\n",
    "    for step in range(MAX_ITERATION):\n",
    "        batch_xs, batch_ys = next(batches)\n",
    "        batch_xs_aug, batch_ys_aug =data_augmentation(batch_xs, batch_ys)\n",
    "\n",
    "        sess.run([extra_update_ops,train_step], feed_dict={x: batch_xs_aug, y_: batch_ys_aug, train_phase: True})\n",
    "\n",
    "        if (step % 500 == 0):\n",
    "            # get training accr\n",
    "            idx = np.arange(len(db_helen['img']['trainset']))    \n",
    "            tb = int(len(db_helen['img']['trainset'])/32)\n",
    "            acc_train= []\n",
    "            for b_idx in range(tb):\n",
    "                tar_idx = idx[(b_idx*32):((b_idx+1)*32)]\n",
    "                t_batch_x = db_helen['img']['trainset'][tar_idx]\n",
    "                t_batch_y = db_helen['pts']['trainset'][tar_idx]\n",
    "            acc_train.append(sess.run(avg_losses, feed_dict={x: t_batch_x, y_: t_batch_y, train_phase: False}))\n",
    "            print(\"[T] Step: %d, loss:%g\" % (step, np.mean(acc_train)))\n",
    "\n",
    "            #tar_idx = idx[(b_idx*32):((b_idx+1)*32)]\n",
    "            t_batch_x = db_helen['img']['testset']#[tar_idx]\n",
    "            t_batch_y = db_helen['pts']['testset']#[tar_idx]\n",
    "            infered_pts, acc_valid= sess.run([tf.reshape(y,shape=(-1,68,2)), avg_losses], feed_dict={x: t_batch_x,\n",
    "                                                                                                     y_: t_batch_y,\n",
    "                                                                                                     train_phase: False})\n",
    "    #             acc_valid.append(valid_loss)\n",
    "            write_result(db_helen['img']['testset'][np.arange(infered_pts.shape[0])], infered_pts, step)\n",
    "\n",
    "            if np.mean(acc_valid) < max_validloss:\n",
    "                saver.save(sess, logs_dir + \"model.ckpt\", step)\n",
    "                print(\"[V*] Step: %d, loss:%g\" % (step, np.mean(acc_valid)))\n",
    "                max_validloss = np.mean(acc_valid)\n",
    "            else:\n",
    "                print(\"[V] Step: %d, loss:%g\" % (step, np.mean(acc_valid)))\n",
    "\n",
    "            hist_train_acc.append(np.mean(acc_train))\n",
    "            hist_valid_acc.append(np.mean(acc_valid))\n",
    "else: # evaluate\n",
    "    t_batch_x = db_helen['img']['testset']#[tar_idx]\n",
    "    t_batch_y = db_helen['pts']['testset']#[tar_idx]\n",
    "    infered_pts, acc_valid= sess.run([tf.reshape(y,shape=(-1,68,2)), avg_losses], feed_dict={x: t_batch_x, y_: t_batch_y, train_phase: False})\n",
    "    norm_error_image = eval_norm_error_image(infered_pts, t_batch_y)\n",
    "    pandas.DataFrame({'loss':norm_error_image}).to_csv('./reg/norm_error_image.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d = 3\n",
    "img = db_helen['img']['testset'][3]\n",
    "pts = t_batch_y = db_helen['pts']['testset'][3]\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "cv2.circle(img,(pts[36,0],pts[36,1]), 2, (255,0,0), -1)\n",
    "cv2.circle(img,(pts[45,0],pts[45,1]), 2, (255,0,0), -1)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
