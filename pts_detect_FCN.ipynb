{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code examples from shekkizh [https://github.com/shekkizh/FCN.tensorflow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mmnet\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc as misc\n",
    "import os, sys\n",
    "from six.moves import urllib\n",
    "import tarfile\n",
    "import zipfile\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import TensorflowUtils as utils\n",
    "import vgg19 as vgg\n",
    "import pickle\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import random\n",
    "import cv2\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''setting'''\n",
    "gpus = [1] # Here I set CUDA to only see one GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=','.join([str(i) for i in gpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load VGG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "MODEL_URL = 'http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat'\n",
    "data_dir = './dataset/'\n",
    "logs_dir = './fcn/logs/'\n",
    "debug = False\n",
    "training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_CLASSESS = int(68 + 1)\n",
    "IMAGE_SIZE = 224\n",
    "learning_rate = 1e-4\n",
    "batch_size = 4\n",
    "MAX_ITERATION = int(1e5 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image, keep_prob):\n",
    "    # load VGG19\n",
    "    model_data = utils.get_model_data(\"Model_zoo/\", MODEL_URL)\n",
    "    # preprocessing\n",
    "    mean = model_data['normalization'][0][0][0]\n",
    "    mean_pixel = np.mean(mean, axis=(0, 1));#print(mean_pixel)\n",
    "    weights = np.squeeze(model_data['layers'])\n",
    "    processed_image = utils.process_image(image, mean_pixel)\n",
    "    \n",
    "    # model\n",
    "    with tf.variable_scope(\"inference\"):\n",
    "        # trnasfer VGG-19\n",
    "        image_net = vgg.vgg_net(weights, processed_image)\n",
    "        conv_final_layer = image_net[\"conv5_3\"];print(\"conv_final_layer\",conv_final_layer)\n",
    "        pool5 = utils.max_pool_2x2(conv_final_layer); print(\"pool5\",pool5)\n",
    "\n",
    "        # conv layers\n",
    "        conv6=tf.contrib.layers.conv2d(pool5, 4096, [7,7], stride=1,padding='SAME')\n",
    "        relu6 = tf.nn.relu(conv6, name=\"relu6\")\n",
    "        if debug:\n",
    "            utils.add_activation_summary(relu6)\n",
    "        relu_dropout6 = tf.nn.dropout(relu6, keep_prob=keep_prob)\n",
    "\n",
    "        conv7=tf.contrib.layers.conv2d(relu_dropout6, 4096, [1,1], stride=1,padding='SAME')\n",
    "        relu7 = tf.nn.relu(conv7, name=\"relu7\")\n",
    "        if debug:\n",
    "            utils.add_activation_summary(relu7)\n",
    "        relu_dropout7 = tf.nn.dropout(relu7, keep_prob=keep_prob)\n",
    "\n",
    "        conv8=tf.contrib.layers.conv2d(relu_dropout7, NUM_OF_CLASSESS, [1,1], stride=1,padding='SAME')\n",
    "\n",
    "        # now to upscale to actual image size\n",
    "        deconv_shape1 = image_net[\"pool4\"].get_shape()\n",
    "        conv_t1 = tf.contrib.layers.conv2d_transpose(conv8, deconv_shape1[3].value, [4, 4],\n",
    "                                                     stride=2, padding='SAME',activation_fn=None)\n",
    "        fuse_1 = tf.add(conv_t1, image_net[\"pool4\"], name=\"fuse_1\");print(\"fuse_1\",fuse_1)\n",
    "\n",
    "        deconv_shape2 = image_net[\"pool3\"].get_shape()\n",
    "        conv_t2 = tf.contrib.layers.conv2d_transpose(fuse_1, deconv_shape2[3].value, [4, 4],\n",
    "                                                     stride=2, padding='SAME',activation_fn=None)\n",
    "        fuse_2 = tf.add(conv_t2, image_net[\"pool3\"], name=\"fuse_2\");print(\"fuse_2\",fuse_2)\n",
    "\n",
    "        conv_t3 = tf.contrib.layers.conv2d_transpose(fuse_2, NUM_OF_CLASSESS, [16, 16],\n",
    "                                                     stride=8, padding='SAME',activation_fn=None);print(\"conv_t3\",conv_t3)\n",
    "\n",
    "        annotation_pred = tf.argmax(conv_t3, axis=3, name=\"prediction\");print(\"annotation_pred\",annotation_pred)\n",
    "        return tf.expand_dims(annotation_pred, dim=3), conv_t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_final_layer Tensor(\"inference/BiasAdd_14:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "pool5 Tensor(\"inference/MaxPool:0\", shape=(?, 7, 7, 512), dtype=float32)\n",
      "fuse_1 Tensor(\"inference/fuse_1:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "fuse_2 Tensor(\"inference/fuse_2:0\", shape=(?, 28, 28, 256), dtype=float32)\n",
      "conv_t3 Tensor(\"inference/Conv2d_transpose_2/BiasAdd:0\", shape=(?, 224, 224, 69), dtype=float32)\n",
      "annotation_pred Tensor(\"inference/prediction:0\", shape=(?, 224, 224), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# setting inputs\n",
    "keep_probability = tf.placeholder(tf.float32, name=\"keep_probabilty\")\n",
    "image = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, 3], name=\"input_image\")\n",
    "annotation = tf.placeholder(tf.int32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, 1], name=\"annotation\")\n",
    "\n",
    "pred_annotation, logits = inference(image, keep_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_parameters 147038154\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "#         print(shape)\n",
    "#         print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "#             print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "#         print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print('total_parameters', total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "loss = tf.reduce_mean((tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                      labels=tf.squeeze(annotation, squeeze_dims=[3]),\n",
    "                                                                      name=\"entropy\")))\n",
    "loss_summary = tf.summary.scalar(\"entropy\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loss_val, var_list):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    grads = optimizer.compute_gradients(loss_val, var_list=var_list)\n",
    "    if debug:\n",
    "        # print(len(var_list))\n",
    "        for grad, var in grads:\n",
    "            utils.add_gradient_summary(grad, var)\n",
    "    return optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up summary op...\n",
      "Setting up Saver...\n"
     ]
    }
   ],
   "source": [
    "trainable_var = tf.trainable_variables()\n",
    "if debug:\n",
    "    for var in trainable_var:\n",
    "        utils.add_to_regularization_and_summary(var)\n",
    "train_op = train(loss, trainable_var)\n",
    "\n",
    "print(\"Setting up summary op...\")\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "print(\"Setting up Saver...\")\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./fcn/logs/model.ckpt-91000\n",
      "Loading sucessfully\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "if (training == False):\n",
    "    ckpt = tf.train.get_checkpoint_state(logs_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print('Loading sucessfully')\n",
    "    else:\n",
    "        print('No checkpoint file found')\n",
    "        raise\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "# create two summary writers to show training loss and validation loss in the same graph\n",
    "# need to create two folders 'train' and 'validation' inside FLAGS.logs_dir\n",
    "train_writer = tf.summary.FileWriter(logs_dir+'/train', sess.graph)\n",
    "validation_writer = tf.summary.FileWriter(logs_dir+'/validation', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(images, pts, rot=(-30, 30), s=(0.7, 1.0)):\n",
    "    keypoints_on_images = []\n",
    "    for idx_img in range(images.shape[0]):\n",
    "        image = images[idx_img]\n",
    "        height, width = image.shape[0:2]\n",
    "        keypoints = []\n",
    "        for p in range(pts.shape[1]):\n",
    "            keypoints.append(ia.Keypoint(x=pts[idx_img,p,0], y=pts[idx_img,p,1]))\n",
    "        keypoints_on_images.append(ia.KeypointsOnImage(keypoints, shape=image.shape))\n",
    "\n",
    "    seq = iaa.Sequential([iaa.Affine(rotate=rot,scale=s)])\n",
    "    seq_det = seq.to_deterministic() # call this for each batch again, NOT only once at the start\n",
    "\n",
    "    # augment keypoints and images\n",
    "    images_aug = seq_det.augment_images(images)\n",
    "    keypoints_aug = seq_det.augment_keypoints(keypoints_on_images)\n",
    "    \n",
    "    pts_aug=[]\n",
    "    for img_idx, keypoints_after in enumerate(keypoints_aug):\n",
    "        img_pts_aug=[]\n",
    "        for kp_idx, keypoint in enumerate(keypoints_after.keypoints):\n",
    "            img_pts_aug.append([round(keypoint.x), round(keypoint.y)])\n",
    "        pts_aug.append(np.asarray(img_pts_aug))\n",
    "\n",
    "    pts_aug = np.asarray(pts_aug).astype(np.int32)\n",
    "    \n",
    "#     print('images_aug', images_aug.shape)\n",
    "#     print('pts_aug', pts_aug.shape)\n",
    "    return images_aug, pts_aug\n",
    "\n",
    "def get_batch(X, Y, batch_size = 32):\n",
    "    # print ('shuffle training dataset')\n",
    "    idx = np.arange(len(X))    \n",
    "    while True:\n",
    "        np.random.shuffle(idx)\n",
    "        tb = int(len(X)/batch_size)\n",
    "        #print('total batches %d' % tb)\n",
    "        for b_idx in range(tb):\n",
    "            tar_idx = idx[(b_idx*batch_size):((b_idx+1)*batch_size)]\n",
    "            t_batch_x = X[tar_idx]\n",
    "            t_batch_y = Y[tar_idx]\n",
    "            # print(b_idx, t_batch_x.shape, t_batch_y.shape)\n",
    "            yield t_batch_x, t_batch_y\n",
    "            \n",
    "def pts2map(ys):\n",
    "    #print(ys.shape)\n",
    "    maps = np.zeros(shape=(ys.shape[0],IMAGE_SIZE,IMAGE_SIZE,1));#print(maps.shape)\n",
    "    for i in range(ys.shape[0]):\n",
    "        for p in range(ys.shape[1]):\n",
    "            if(((ys[i,p,0]) < IMAGE_SIZE) & ((ys[i,p,1]) < IMAGE_SIZE) & (ys[i,p,0] > -1) & (ys[i,p,1] > -1)):\n",
    "                maps[i,ys[i,p,0],  ys[i,p,1],0] = p+1 # shift label from 0:68 to 1:69\n",
    "    return maps\n",
    "\n",
    "def map2pts(pts_maps):\n",
    "    b_idxs = []\n",
    "    for b in range(pts_maps.shape[0]): \n",
    "        idxs = []\n",
    "        for p in range(pts_maps.shape[3]): \n",
    "            idx = np.unravel_index(np.argmax(pts_maps[b,...,p], axis=None), pts_maps[b,...,p].shape)\n",
    "            idxs.append(idx)\n",
    "        b_idxs.append(np.asarray(idxs))\n",
    "    b_idxs = np.asarray(b_idxs);#print(b_idxs.shape)\n",
    "    b_idxs = b_idxs[:,1:,:]\n",
    "    return b_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['img', 'pts'])\n",
      "dict_keys(['testset', 'trainset'])\n",
      "(2000, 68, 2)\n",
      "(2000, 224, 224, 3)\n",
      "(330, 68, 2)\n",
      "(330, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "db_helen = pickle.load(open(data_dir+\"HELEN.pickle\", \"rb\" ) )\n",
    "# print the data structure\n",
    "print(db_helen.keys())\n",
    "print(db_helen['pts'].keys())\n",
    "# print the shape of tratining set\n",
    "print(db_helen['pts']['trainset'].shape)\n",
    "print(db_helen['img']['trainset'].shape)\n",
    "# print the shape of testing set\n",
    "print(db_helen['pts']['testset'].shape)\n",
    "print(db_helen['img']['testset'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declear data iterator\n",
    "train_batches = get_batch(db_helen['img']['trainset'], db_helen['pts']['trainset'], batch_size = batch_size)\n",
    "valid_batches = get_batch(db_helen['img']['testset'], db_helen['pts']['testset'], batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write image to \n",
    "def write_result(batch_xs_valid, pts_maps, iter_num):\n",
    "    b = random.randint(0, batch_xs_valid.shape[0]-1)\n",
    "    img = batch_xs_valid[b].copy()\n",
    "    pts = map2pts(pts_maps)[b] #print(pts)\n",
    "#     print(img.shape)\n",
    "#     print(pts.shape)\n",
    "    for p in range(pts.shape[0]):\n",
    "        #print(\"p\",p, pts[p+1,0],pts[p+1,1])\n",
    "        cv2.circle(img,(pts[p,0],pts[p,1]), 2, (255,0,0), -1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite('./fcn/imgs/infer_'+str(iter_num)+'.png', img) \n",
    "\n",
    "def eval_norm_error_image(infer, gt):\n",
    "    # loss of all landmarks\n",
    "    l2d = np.sum(np.sqrt(np.sum(np.square(infer-gt),axis=2)), axis=1)\n",
    "    # distance of eye corners\n",
    "    cd = np.sqrt(np.sum(np.square(gt[:,45,:]-gt[:,36,:]),axis=1))\n",
    "    norm_error_image = l2d/cd/68\n",
    "    return norm_error_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if training ==True:\n",
    "    max_validloss = 99999\n",
    "    for itr in range(MAX_ITERATION):\n",
    "        # prepare training input\n",
    "        batch_xs, batch_ys = next(train_batches)\n",
    "        batch_xs_aug, batch_ys_aug = data_augmentation(batch_xs, batch_ys)\n",
    "        batch_ymap_aug = pts2map(batch_ys_aug)\n",
    "\n",
    "        feed_dict = {image: batch_xs_aug, annotation: batch_ymap_aug, keep_probability: 0.85}\n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "\n",
    "        if itr % 500 == 0:\n",
    "            train_loss, summary_str = sess.run([loss, loss_summary], feed_dict=feed_dict)\n",
    "            print(\"[T] Step: %d, loss:%g\" % (itr, train_loss))\n",
    "            train_writer.add_summary(summary_str, itr)\n",
    "        # validation\n",
    "        if itr % 1000 == 0:\n",
    "            # prepare inputs\n",
    "            batch_xs_valid, batch_ys_valid = next(valid_batches)\n",
    "            batch_ymap_valid = pts2map(batch_ys_valid);# print(batch_ymap_valid.shape)\n",
    "\n",
    "            feed_dict = {image: batch_xs_valid, annotation: batch_ymap_valid, keep_probability: 1.0}\n",
    "            valid_loss, pts_maps, summary_sva=sess.run([loss, tf.nn.softmax(logits), loss_summary], feed_dict=feed_dict)\n",
    "            # write result figure to the imgs/\n",
    "            write_result(batch_xs_valid, pts_maps, itr)\n",
    "            # save validation log\n",
    "            validation_writer.add_summary(summary_sva, itr)\n",
    "            # save the ckpt if reachings better loss\n",
    "            if valid_loss < max_validloss:\n",
    "                saver.save(sess, logs_dir + \"model.ckpt\", itr)\n",
    "                print(\"[V*] Step: %d, loss:%g\" % (itr, valid_loss))\n",
    "                max_validloss = valid_loss\n",
    "            else:\n",
    "                print(\"[V] Step: %d, loss:%g\" % (itr, valid_loss))\n",
    "else:\n",
    "    testing_batch = 30\n",
    "    neis = []\n",
    "    for t in range(int(db_helen['img']['testset'].shape[0]/testing_batch)):\n",
    "        t_batch_x = db_helen['img']['testset'][(t*testing_batch):((t+1)*testing_batch)]\n",
    "        t_batch_y = db_helen['pts']['testset'][(t*testing_batch):((t+1)*testing_batch)]\n",
    "        feed_dict = {image: t_batch_x, keep_probability: 1.0}\n",
    "        pts_maps=sess.run(tf.nn.softmax(logits), feed_dict=feed_dict)\n",
    "        infered_pts = map2pts(pts_maps)\n",
    "        norm_error_image = eval_norm_error_image(infered_pts, t_batch_y)\n",
    "        neis.append(norm_error_image)\n",
    "    neis = np.reshape(np.asarray(neis),newshape=-1)\n",
    "    pandas.DataFrame({'loss':neis}).to_csv('./fcn/norm_error_image.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
